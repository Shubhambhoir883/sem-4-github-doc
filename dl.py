# -*- coding: utf-8 -*-
"""DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BzqVtR_oR4W5s2kNIAYPCWDnD0XNdyiO
"""

# DL PRACT 1 A
# Create Vector, Matrix and Tensor

import numpy as np
import tensorflow as tf
x=np.array([1,2,3,4])
print("Create a Vector: ",x)
print("\n")
A=np.array([[1,2],[3,4],[5,6]])
print("Create a Matrix: \n",A)
print("\n")
tensor_A=tf.constant([[1,2]],dtype=tf.int32)
print("Create a Tensor: ",tensor_A)
print("\n")

# DL PRACT 1 B
# Multiplication of two: Vector, Matrix and Tensor

A=np.array([[1,2],[3,4],[5,6]])
print("A= ",A)
print("\n")
B=np.array([[2,5],[7,4],[4,3]])
print("B= ",B)
print("\n")
C=A*B
print("Multiplication of two Matrix: \n",C)
print("\n")
x=np.array([1,2,3,4])
y=np.array([5,6,7,8])
z=x*y
print("Multiplication of two Vector: ",z)
print("\n")
tensor_A=tf.constant([[4,2]],dtype=tf.int32)
print("A: ",tensor_A)
tensor_B=tf.constant([[7,4]],dtype=tf.int32)
print("B: ",tensor_B,"\n")
tensor_multiply=tf.multiply(tensor_A,tensor_B)
print("Multiplication of two Tensor: ",tensor_multiply)
print("\n")

# DL PRACT 1 C
# Addition of two : Vector, Matrix and Tensor

x=np.array([1,2,3,4])
y=np.array([5,6,7,8])
z=x+y
print("Addition of two Matrix: ",z)
print("\n")
A=np.array([[1,2],[3,4],[5,6]])
B=np.array([[2,5],[7,4],[4,3]])
C=A*B
print("Addition of two Vector: \n",C)
print("\n")
tensor_add=tf.add(tensor_A,tensor_B)
print("Addition of two Tensor: ",tensor_add)
print("\n")

# DL 1 PRACT 1 D
# Multiply Matrix with Vector

x=np.array([1,2,3,7,3,5,2])
y=np.array([[1],[3],[5],[7],[8],[8],[2]])
c=x*y
print("Multiplication of Vector and Matrix: \n",c)
print("\n")

# DL 1 PRACT 1 E
# Matrix Dot product and Matrix Inverse

U=[2,-3]
V=[1,3]
dotproduct=np.dot(U,V)
print("Matrix dot product: ",dotproduct)
print("\n")
A=np.array([[6,1,1],
[4,-2,5],
[2,8,7]])
print("Inverse of Matrix: \n",np.linalg.inv(A))
print("\n")

# DL PRACT 2
# Performing matrix multiplication and finding Eigen vectors and Eigen values using TensorFlow

import tensorflow as tf
x= tf.constant([1,2,3,4,5,6],shape=[2,3])
print(x)
y= tf.constant([7,8,9,10,11,12],shape=[3,2])
print(y)
z= tf.matmul(x,y)
print("\n")
print("Multiplying the matrices: ")
print(z)
print("\n")
e_matrix_A=tf.random.uniform([2,2],minval=3,maxval=10,dtype=tf.float32,name="matrixA ")
print("\n")
print("Matrix A: \n{}\n\n".format(e_matrix_A))
eigan_values_A,eigan_vectors_A=tf.linalg.eigh(e_matrix_A)
print("Eigan Vectors: \n{}\n\nEigan Values:\n{}\n\n".format(eigan_vectors_A,eigan_values_A))

pip install scikeras

# DL PRACT 3
# : Implementing deep neural network for performing binary classification task.

import pandas as pd
from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold

dataframe = read_csv('/content/sonar.csv', header=None)

dataset = dataframe.apply(pd.to_numeric, errors='coerce').fillna(0).values

X = dataset[:,0:60].astype(float)
Y = dataset[:,60]

encoder = LabelEncoder()
encoder.fit(Y)
encoded_y = encoder.transform(Y)

def create_baseline():
    model = Sequential()
    model.add(Dense(60, input_dim=60, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

estimator = KerasClassifier(model=create_baseline, epochs=10, batch_size=5, verbose=0)

kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(estimator, X, encoded_y, cv=kfold)

print("\n")
print("Baseline: %.2f%% (%.2f%%)" % (results.mean() * 100, results.std() * 100))

# DL PRACT 4
# Using deep feed forward network with two hidden layers for performing classification and predicting the probability of class.

import numpy as np
from sklearn.datasets import load_wine
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from keras.layers import Dense, Input, concatenate, Dropout
from keras.models import Model
from tensorflow.keras import optimizers

optimizers.RMSprop
optimizers.Adam

dataset = load_wine()
ensemble_num = 10
bootstrap_size = 0.8
training_size = 0.8
num_hidden_neurons = 10
dropout = 0.25
epochs = 100
batch = 10
temp = []

scaler = MinMaxScaler()
one_hot = OneHotEncoder()
dataset['data'] = scaler.fit_transform(dataset['data'])
dataset['target'] = one_hot.fit_transform(np.reshape(dataset['target'], (-1, 1))).toarray()

for i in range(len(dataset.data)):
    temp.append([dataset['data'][i], np.array(dataset['target'][i])])
temp = np.array(temp, dtype=object)
np.random.shuffle(temp)

stop = int(training_size * len(dataset.data))
train_X = np.array([x for x in temp[:stop, 0]])
train_Y = np.array([x for x in temp[:stop, 1]])
test_X = np.array([x for x in temp[stop:, 0]])
test_Y = np.array([x for x in temp[stop:, 1]])

num_hidden_neurons = 64
sub_net_outputs = []
sub_net_inputs = []

for i in range(ensemble_num):
    net_input = Input(shape=(train_X.shape[1],))
    sub_net_inputs.append(net_input)
    Y = Dense(num_hidden_neurons)(net_input)
    Y = Dense(num_hidden_neurons)(Y)
    Y = Dropout(dropout)(Y)
    sub_net_outputs.append(Y)


Y = concatenate(sub_net_outputs)
Y = Dense(train_Y[0].shape[0], activation='softmax')(Y)
model = Model(inputs=sub_net_inputs, outputs=Y)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

print("\n")
print("7_Aditya Hadap")
print("Begin training...")
model.fit([train_X] * ensemble_num, train_Y, validation_data=([test_X] * ensemble_num, test_Y), epochs=epochs, batch_size=batch)
print("Training complete...")

np.set_printoptions(precision=2, suppress=True)
for i in range(len(test_X)):
    print("Prediction: " + str(model.predict([test_X[i].reshape(1, test_X[i].shape[0])] * ensemble_num)) + " | True: " + str(test_Y[i]))

# DL PRACT 5
# Evaluating feed forward deep network for multiclass Classification using K-Fold cross-validation.

import pandas
from keras.models import Sequential
from keras.layers import Dense
from scikeras.wrappers import KerasClassifier
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

# Load dataset and skip the first column (ID column)
dataframe = pandas.read_csv("/content/Iris (1).csv")
dataset = dataframe.values
X = dataset[:, 1:5].astype(float)  # Assuming the first column is an ID and skipping it
Y = dataset[:, 5]  # Adjust this if there are headers

# Encode labels
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
dummy_y = to_categorical(encoded_Y)

# Define baseline model
def baseline_model():
    model = Sequential()
    model.add(Dense(8, input_dim=4, activation='relu'))
    model.add(Dense(3, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Wrap model using KerasClassifier
estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)

# Evaluate model using cross-validation
kfold = KFold(n_splits=10, shuffle=True)
results = cross_val_score(estimator, X, dummy_y, cv=kfold)

# Print results
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean() * 100, results.std() * 100))

# DL PRACT 6
# Implementation of convolutional neural network to predict numbers from number images.

from keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten
import matplotlib.pyplot as plt
import numpy as np
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
plt.imshow(X_train[0])
plt.show
print(X_train[0].shape)
X_train=X_train.reshape(60000,28,28,1)
X_test=X_test.reshape(10000,28,28,1)
Y_train=to_categorical(Y_train)
Y_test=to_categorical(Y_test)
Y_train[0]
print(Y_train[0])
model=Sequential()
model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))
model.add(Conv2D(32, kernel_size=3, activation='relu'))
model.add(Flatten())
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=1)
print(model.predict(X_test[:4]))
print(Y_test[:4])

# DL PRACT 7
# Performing encoding and decoding of images using deep autoencoder.

import keras
from keras import layers
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
encoding_dim=32
input_img=keras.Input(shape=(784,))
encoded=layers.Dense(encoding_dim,activation='relu')(input_img)
decoded=layers.Dense(784,activation='sigmoid')(encoded)
autoencoder=keras.Model(input_img,decoded)
encoder=keras.Model(input_img,encoded)
encoded_input=keras.Input(shape=(encoding_dim,))
decoder_layer=autoencoder.layers[-1]
decoder=keras.Model(encoded_input,decoder_layer(encoded_input))
autoencoder.compile(optimizer='adam',loss='binary_crossentropy')
(x_train,_),(x_test,_)=mnist.load_data()
x_train=x_train.astype('float32')/255.
x_test=x_test.astype('float32')/25
x_train=x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))
x_test=x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))
print(x_train.shape)
print(x_test.shape)
autoencoder.fit(x_train,x_train,epochs=13,batch_size=256,shuffle=True,validation_data=(x_test,x_test))
encoded_imgs=encoder.predict(x_test)
decoded_imgs=decoder.predict(encoded_imgs)
n=10
plt.figure(figsize=(20,4))
for i in range(n):
  ax=plt.subplot(2,n,i+1)
  plt.imshow(x_test[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
  ax=plt.subplot(2,n,i+1+n)
  plt.imshow(decoded_imgs[i].reshape(28,28))
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
plt.show()

# DL PRACT 8
# Denoising of images using Autoencoder.

import keras
from keras.datasets import mnist
from keras import layers
import numpy as np
from keras.callbacks import TensorBoard
import matplotlib.pyplot as plt

# Load MNIST dataset
(X_train, _), (X_test, _) = mnist.load_data()

# Normalize the data and reshape it
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.
X_train = np.reshape(X_train, (len(X_train), 28, 28, 1))
X_test = np.reshape(X_test, (len(X_test), 28, 28, 1))

# Add noise to the images
noise_factor = 0.5
X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)
X_train_noisy = np.clip(X_train_noisy, 0., 1.)
X_test_noisy = np.clip(X_test_noisy, 0., 1.)

# Display noisy images
n = 10
plt.figure(figsize=(20, 2))
for i in range(1, n + 1):
    ax = plt.subplot(1, n, i)
    plt.imshow(X_test_noisy[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

# Define the autoencoder model
input_img = keras.Input(shape=(28, 28, 1))

# Encoder
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
encoded = layers.MaxPooling2D((2, 2), padding='same')(x)

# Decoder
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = layers.UpSampling2D((2, 2))(x)
decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

# Compile the model
autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Train the autoencoder
autoencoder.fit(X_train_noisy, X_train,
                epochs=3,
                batch_size=328,
                shuffle=True,
                validation_data=(X_test_noisy, X_test),
                callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])

# Predict and display the denoised images
predictions = autoencoder.predict(X_test_noisy)
m = 10
plt.figure(figsize=(20, 2))
for i in range(1, m + 1):
    ax = plt.subplot(1, m, i)
    plt.imshow(predictions[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()